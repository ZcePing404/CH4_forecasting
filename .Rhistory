#for lines that match a regular expression
minimax.regret = row.names(payoff)[gr1]; minimax.regret
d <- data.frame(which.min(rrowmax), rownames(payoff)[which.min(rrowmax)],
min(rrowmax))
dimnames(d) <- list('', c('strategyNumber', 'strategyName', 'value'))
list(expected=rrowmax, decision=d)
decisions =
cbind(rbind(maximax,maximin,hurwicz,equally.like,minimax.regret),
rbind(max(rowmax),max(rowmin),max(hurz),max(rowavg),min(rrowmax)))
#dimnames(decisions)[[2]] = c("Decisions by Various Criteria so far")
dimnames(decisions)[[2]] = c("strategyName", "value")
print(decisions)
# Example 3.7
## 6. EMV
sum(prob)
expected.value = payoff %*% prob; expected.value
maxev = max(expected.value)
print(c("maximum Expected Value",maxev))
gr1 = grep(max(expected.value),expected.value); gr1
max.Exp.Value = row.names(payoff)[gr1]
max.Exp.Value
d <- data.frame(which.max(expected.value),
rownames(payoff)[which.max(expected.value)], max(expected.value))
dimnames(d) <- list('', c('strategyNumber', 'strategyName', 'value'))
list(expected=expected.value, decision=d)
# Example 3.9
## 7. EOL
expected.regret = regret %*% prob; expected.regret
miner = min(expected.regret)
print(c("minimum Expected Regret",miner))
gr1 = grep(min(expected.regret),expected.regret); gr1
min.Exp.Regret = row.names(payoff)[gr1]
min.Exp.Regret
d <- data.frame(which.min(expected.regret),
rownames(payoff)[which.min(expected.regret)], min(expected.regret))
dimnames(d) <- list('', c('strategyNumber', 'strategyName', 'value'))
list(expected=expected.regret, decision=d)
# Summary
## Summary Decisions by Expected Value Criteria
decisions = cbind(rbind(max.Exp.Value,min.Exp.Regret),
rbind(maxev,miner))
#dimnames(decisions)[[2]] = c("Decisions by Various Criteria so far")
dimnames(decisions)[[2]] = c("strategyName", "value")
print(decisions)
# Example 3.10
## 8. EVPI
max.certain = t(prob) %*% colmax; max.certain
max(expected.value)
cost.of.uncertainty = max.certain - max(expected.value)
cost.of.uncertainty
data <- read.csv("Retail_Prediction/avocado.csv")
install.packages("keras")
library(keras)
# Generate sample time series (sine wave)
set.seed(123)
time <- 1:200
data <- sin(time * 0.1) + rnorm(200, sd = 0.1)
# Prepare supervised data (windowed sequences)
lag <- 5   # number of time steps to look back
X <- t(sapply(1:(length(data)-lag), function(i) data[i:(i+lag-1)]))
y <- data[(lag+1):length(data)]
# Train-test split
train_size <- round(0.8 * nrow(X))
X_train <- X[1:train_size, 1:lag]
y_train <- y[1:train_size]
X_test  <- X[(train_size+1):nrow(X), 1:lag]
y_test  <- y[(train_size+1):length(y)]
# Reshape for LSTM [samples, timesteps, features]
X_train <- array(X_train, dim = c(nrow(X_train), lag, 1))
X_test  <- array(X_test,  dim = c(nrow(X_test), lag, 1))
train_size <- round(0.8 * nrow(X))
X_train <- X[1:train_size, 1:lag]
y_train <- y[1:train_size]
X_test  <- X[(train_size+1):nrow(X), 1:lag]
y_test  <- y[(train_size+1):length(y)]
# Reshape for LSTM [samples, timesteps, features]
X_train <- array(X_train, dim = c(nrow(X_train), lag, 1))
X_test  <- array(X_test,  dim = c(nrow(X_test), lag, 1))
# Build LSTM model
model <- keras_model_sequential() %>%
layer_lstm(units = 50, input_shape = c(lag, 1)) %>%
layer_dense(units = 1)
install_tensorflow()
library(tensorflow)
install_tensorflow()
install.packages("tensorflow")
install.packages("tensorflow")
library(tensorflow)
# Compile model
model %>% compile(
loss = "mean_squared_error",
optimizer = "adam"
)
# Build LSTM model
model <- keras_model_sequential() %>%
layer_lstm(units = 50, input_shape = c(lag, 1)) %>%
layer_dense(units = 1)
# Build LSTM model
model <- keras_model_sequential() %>%
layer_lstm(units = 50, input_shape = c(lag, 1)) %>%
layer_dense(units = 1)
py_require("tensorflow")
# Build LSTM model
model <- keras_model_sequential() %>%
layer_lstm(units = 50, input_shape = c(lag, 1)) %>%
layer_dense(units = 1)
reticulate::py_last_error()
model <- keras_model_sequential() %>%
layer_lstm(units = 50, input_shape = c(lag, 1), return_sequences = FALSE) %>%
layer_dense(units = 1)
model <- keras_model_sequential() %>%
layer_lstm(units = 50, input_shape = c(lag, 1)) %>%
layer_dense(units = 1)
py_require("tensorflow")
# Build LSTM model
model <- keras_model_sequential() %>%
layer_lstm(units = 50, input_shape = c(lag, 1)) %>%
layer_dense(units = 1)
# Build LSTM model
model <- keras_model_sequential() %>%
layer_lstm(units = 50, input_shape = c(lag, 1), return_sequences = FALSE) %>%
layer_dense(units = 1)
# Build LSTM model
model <- keras_model_sequential() %>%
layer_input(shape = c(lag, 1)) %>%
layer_lstm(units = 50) %>%
layer_dense(units = 1)
# Build LSTM model
model <- keras_model_sequential() %>%
layer_lstm(units = 50, input_shape = c(lag, 1), return_sequences = FALSE) %>%
layer_dense(units = 1)
install.packages("keras")
library(tensorflow)
library(keras)
# Generate sample time series (sine wave)
set.seed(123)
time <- 1:200
data <- sin(time * 0.1) + rnorm(200, sd = 0.1)
# Prepare supervised data (windowed sequences)
lag <- 5   # number of time steps to look back
X <- t(sapply(1:(length(data)-lag), function(i) data[i:(i+lag-1)]))
y <- data[(lag+1):length(data)]
# Train-test split
train_size <- round(0.8 * nrow(X))
X_train <- X[1:train_size, 1:lag]
y_train <- y[1:train_size]
X_test  <- X[(train_size+1):nrow(X), 1:lag]
y_test  <- y[(train_size+1):length(y)]
# Reshape for LSTM [samples, timesteps, features]
X_train <- array(X_train, dim = c(nrow(X_train), lag, 1))
X_test  <- array(X_test,  dim = c(nrow(X_test), lag, 1))
py_require("tensorflow")
# Build LSTM model
model <- keras_model_sequential() %>%
layer_lstm(units = 50, input_shape = c(lag, 1), return_sequences = FALSE) %>%
layer_dense(units = 1)
library(tensorflow)
library(keras)
# Generate sample time series (sine wave)
set.seed(123)
time <- 1:200
data <- sin(time * 0.1) + rnorm(200, sd = 0.1)
# Prepare supervised data (windowed sequences)
lag <- 5   # number of time steps to look back
X <- t(sapply(1:(length(data)-lag), function(i) data[i:(i+lag-1)]))
y <- data[(lag+1):length(data)]
# Train-test split
train_size <- round(0.8 * nrow(X))
X_train <- X[1:train_size, 1:lag]
y_train <- y[1:train_size]
X_test  <- X[(train_size+1):nrow(X), 1:lag]
y_test  <- y[(train_size+1):length(y)]
# Reshape for LSTM [samples, timesteps, features]
X_train <- array(X_train, dim = c(nrow(X_train), lag, 1))
X_test  <- array(X_test,  dim = c(nrow(X_test), lag, 1))
py_require("tensorflow")
# Build LSTM model
model <- keras_model_sequential() %>%
layer_lstm(units = 50, input_shape = c(lag, 1), return_sequences = FALSE) %>%
layer_dense(units = 1)
py_require("tensorflow")
# Build LSTM model
model <- keras_model_sequential() %>%
layer_input(shape = c(lag, 1)) %>%
layer_lstm(units = 50) %>%
layer_dense(units = 1)
# Build LSTM model
model <- keras_model_sequential() %>%
layer_lstm(units = 50) %>%
layer_dense(units = 1)
install_tensorflow(version = "2.15")
py_require("tensorflow")
install_tensorflow(version = "2.15")
py_require("tensorflow")
install_tensorflow(version = "2.15")
py_require("tensorflow")
install_tensorflow(version = "2.15")
library(tensorflow)
library(keras)
# Generate sample time series (sine wave)
set.seed(123)
time <- 1:200
data <- sin(time * 0.1) + rnorm(200, sd = 0.1)
# Prepare supervised data (windowed sequences)
lag <- 5   # number of time steps to look back
X <- t(sapply(1:(length(data)-lag), function(i) data[i:(i+lag-1)]))
y <- data[(lag+1):length(data)]
# Train-test split
train_size <- round(0.8 * nrow(X))
X_train <- X[1:train_size, 1:lag]
y_train <- y[1:train_size]
X_test  <- X[(train_size+1):nrow(X), 1:lag]
y_test  <- y[(train_size+1):length(y)]
# Reshape for LSTM [samples, timesteps, features]
X_train <- array(X_train, dim = c(nrow(X_train), lag, 1))
X_test  <- array(X_test,  dim = c(nrow(X_test), lag, 1))
py_require("tensorflow")
# Build LSTM model
model <- keras_model_sequential() %>%
layer_input(shape = c(lag, 1)) %>%
layer_lstm(units = 50) %>%
layer_dense(units = 1)
# Build LSTM model
model <- keras_model_sequential() %>%
layer_lstm(units = 50) %>%
layer_dense(units = 1)
remove.packages("keras")
install.packages("keras3") # or remotes::install_github("rstudio/keras")
library(keras3)
library(keras3)
# Build LSTM model
model <- keras_model_sequential() %>%
layer_lstm(units = 50) %>%
layer_dense(units = 1)
# Compile model
model %>% compile(
loss = "mean_squared_error",
optimizer = "adam"
)
# Fit model
history <- model %>% fit(
X_train, y_train,
epochs = 50,
batch_size = 16,
validation_split = 0.2,
verbose = 0
)
# Make predictions
pred <- model %>% predict(X_test)
# Plot results
plot(y_test, type = "l", col = "blue", main = "LSTM Forecast vs Actual",
ylab = "Value", xlab = "Time")
lines(pred, col = "red")
legend("topleft", legend = c("Actual", "Predicted"), col = c("blue","red"), lty = 1)
plot(time, data, type = "l", col = "blue",
main = "Simulated Time Series",
xlab = "Time", ylab = "Value")
plot(time, data[1:length(y)], type = "l", col = "black", lwd = 2,
main = "Train-Test Split", xlab = "Time", ylab = "Value")
# Plot the whole dataset
plot(time, data[1:length(y)], type = "l", col = "black", lwd = 2,
main = "Train-Test Split", xlab = "Time", ylab = "Value")
length(x)
length(y)
length(X)
length(Y)
length(X)
length(Y)
length(y)
length(X)
length(y)
View(X)
# Plot the original data
plot(time, data, type = "l", col = "grey",
main = "Train-Test Split on Time Series",
xlab = "Time", ylab = "Value")
# Overlay training part
lines(time[(lag+1):(lag+length(y_train))], y_train,
col = "blue", lwd = 2)
# Overlay test part
lines(time[(lag+length(y_train)+1):(lag+length(y_train)+length(y_test))], y_test,
col = "red", lwd = 2)
# Add legend
legend("topright", legend = c("Original", "Train", "Test"),
col = c("grey", "blue", "red"), lty = 1, lwd = 2)
py_require("tensorflow")
# Build LSTM model
model <- keras_model_sequential() %>%
layer_lstm(units = 50) %>%
layer_dense(units = 1)
# Compile model
model %>% compile(
loss = "mean_squared_error",
optimizer = "adam"
)
# Fit model
history <- model %>% fit(
X_train, y_train,
epochs = 50,
batch_size = 16,
validation_split = 0.2,
verbose = 0
)
# Make predictions
pred <- model %>% predict(X_test)
# Plot results
plot(y_test, type = "l", col = "blue", main = "LSTM Forecast vs Actual",
ylab = "Value", xlab = "Time")
lines(pred, col = "red")
legend("topleft", legend = c("Actual", "Predicted"), col = c("blue","red"), lty = 1)
legend("topright", legend = c("Actual", "Predicted"), col = c("blue","red"), lty = 1)
legend("topleft", legend = c("Actual", "Predicted"), col = c("blue","red"), lty = 1)
library(tensorflow)
library(keras3)
# Generate sample time series (sine wave)
set.seed(123)
time <- 1:200
data <- sin(time * 0.1) + rnorm(200, sd = 0.1)
plot(time, data, type = "l", col = "blue",
main = "Simulated Time Series",
xlab = "Time", ylab = "Value")
# Prepare supervised data (windowed sequences)
lag <- 5   # number of time steps to look back
X <- t(sapply(1:(length(data)-lag), function(i) data[i:(i+lag-1)]))
y <- data[(lag+1):length(data)]
# Train-test split
train_size <- round(0.8 * nrow(X))
X_train <- X[1:train_size, 1:lag]
y_train <- y[1:train_size]
X_test  <- X[(train_size+1):nrow(X), 1:lag]
y_test  <- y[(train_size+1):length(y)]
# Reshape for LSTM [samples, timesteps, features]
X_train <- array(X_train, dim = c(nrow(X_train), lag, 1))
X_test  <- array(X_test,  dim = c(nrow(X_test), lag, 1))
# Plot the original data
plot(time, data, type = "l", col = "grey",
main = "Train-Test Split on Time Series",
xlab = "Time", ylab = "Value")
# Overlay training part
lines(time[(lag+1):(lag+length(y_train))], y_train,
col = "blue", lwd = 2)
# Overlay test part
lines(time[(lag+length(y_train)+1):(lag+length(y_train)+length(y_test))], y_test,
col = "red", lwd = 2)
# Add legend
legend("topright", legend = c("Original", "Train", "Test"),
col = c("grey", "blue", "red"), lty = 1, lwd = 2)
py_require("tensorflow")
# Build LSTM model
model <- keras_model_sequential() %>%
layer_lstm(units = 50) %>%
layer_dense(units = 1)
# Compile model
model %>% compile(
loss = "mean_squared_error",
optimizer = "adam"
)
# Fit model
history <- model %>% fit(
X_train, y_train,
epochs = 50,
batch_size = 16,
validation_split = 0.2,
verbose = 0
)
# Make predictions
pred <- model %>% predict(X_test)
# Plot results
plot(y_test, type = "l", col = "blue", main = "LSTM Forecast vs Actual",
ylab = "Value", xlab = "Time")
lines(pred, col = "red")
legend("topleft", legend = c("Actual", "Predicted"), col = c("blue","red"), lty = 1)
library(tensorflow)
library(keras3)
# Generate sample time series (sine wave)
set.seed(123)
time <- 1:200
data <- sin(time * 0.1) + rnorm(200, sd = 0.1)
plot(time, data, type = "l", col = "blue",
main = "Simulated Time Series",
xlab = "Time", ylab = "Value")
# Prepare supervised data (windowed sequences)
lag <- 5   # number of time steps to look back
X <- t(sapply(1:(length(data)-lag), function(i) data[i:(i+lag-1)]))
y <- data[(lag+1):length(data)]
# Train-test split
train_size <- round(0.8 * nrow(X))
X_train <- X[1:train_size, 1:lag]
y_train <- y[1:train_size]
X_test  <- X[(train_size+1):nrow(X), 1:lag]
y_test  <- y[(train_size+1):length(y)]
# Reshape for LSTM [samples, timesteps, features]
X_train <- array(X_train, dim = c(nrow(X_train), lag, 1))
X_test  <- array(X_test,  dim = c(nrow(X_test), lag, 1))
# Plot the original data
plot(time, data, type = "l", col = "grey",
main = "Train-Test Split on Time Series",
xlab = "Time", ylab = "Value")
# Overlay training part
lines(time[(lag+1):(lag+length(y_train))], y_train,
col = "blue", lwd = 2)
# Overlay test part
lines(time[(lag+length(y_train)+1):(lag+length(y_train)+length(y_test))], y_test,
col = "red", lwd = 2)
# Add legend
legend("topleft", legend = c("Original", "Train", "Test"),
col = c("grey", "blue", "red"), lty = 1, lwd = 2)
py_require("tensorflow")
# Build LSTM model
model <- keras_model_sequential() %>%
layer_lstm(units = 50) %>%
layer_dense(units = 1)
# Compile model
model %>% compile(
loss = "mean_squared_error",
optimizer = "adam"
)
# Fit model
history <- model %>% fit(
X_train, y_train,
epochs = 50,
batch_size = 16,
validation_split = 0.2,
verbose = 0
)
# Make predictions
pred <- model %>% predict(X_test)
# Plot results
plot(y_test, type = "l", col = "blue", main = "LSTM Forecast vs Actual",
ylab = "Value", xlab = "Time")
lines(pred, col = "red")
legend("topleft", legend = c("Actual", "Predicted"), col = c("blue","red"), lty = 1)
# install.packages("tuner")
library(readr)
install.packages("tuner")
library(tuner)
install.packages("tuneR")
library(tuner)
install.packages("tuner")
R.version.string
install.packages("installr")
library(installr)
updateR()
install.packages("remotes")
remotes::install_github("EagerAI/kerastuneR")
library(kerastuneR)
install_kerastuner()
gc()
gc()
gc()
setwd("C:\\Users\\enton\\Project\\R\\CO2_forecasting")
library(readr)     # to read csv
library(dplyr)     # for data manipulation
library(lubridate) # to handle dates
library(forecast)  # ARIMA forecasting
library(ggplot2)   # plotting
library(tseries)
library(urca)
library(zoo)
library(prophet)
library(Metrics)
source("preprocessing.R")
source("stationary_test.R")
source("differencing_method.R")
source("ARIMA.R")
source("HtWinters.R")
source("Prophet.R")
# Read dataset
df <- read_csv("co2_concentration.csv")
print(df)
str(df)
processed_data <- preprocess_data(df)
df_clean = processed_data$df
ts_data = processed_data$ts_data
# show the resulting monthly data frame to check the output
print(df_clean)
# Plot time series
ggplot(df_clean, aes(x = date, y = average)) +
geom_line(color = "steelblue") +
labs(title = "Monthly Avg CO2 Concentration from Jan 2010 to Dec 2023",
x = "date", y = "Avg Concentration")
# ggplot(df_clean, aes(x = average)) +
#   geom_histogram(fill = "skyblue", bins = 30, color = "black") +
#   labs(title = "Distribution of CO2 concentration",
#        x = "Avg Concentration", y = "Count")
stationary_test(df_clean)
differencing_method(df_clean)
Data <- df_clean$average
train_size <- floor(0.80 * length(Data))
train <- head(Data, train_size)
min_date <- min(df_clean$date)
min_year <- as.numeric(format(min_date, "%Y"))
min_month <- as.numeric(format(min_date, "%m"))
test  <- tail(Data, length(Data) - train_size)
ts_train <- ts(train,
frequency = 12,
start = c(min_year, min_month))
# Get the last date of the training set
last_train_date <- max(df_clean$date[1:length(train)])
first_test_date <- last_train_date %m+% months(1) # Find the date of the first observation in the test set
# Use the year and month of the first test date to set the start of the ts object
ts_test <- ts(test,
start = c(year(first_test_date), month(first_test_date)),
frequency = 12)
checkresiduals(ts_train)
ARIMA_method(ts_train, ts_test)
# Read dataset
df <- read_csv("nitrous_oxide_concentration")
# Read dataset
df <- read_csv("nitrous_oxide_concentration.csv")
print(df)
str(df)
