lines(time[(lag+length(y_train)+1):(lag+length(y_train)+length(y_test))], y_test,
col = "red", lwd = 2)
# Add legend
legend("topright", legend = c("Original", "Train", "Test"),
col = c("grey", "blue", "red"), lty = 1, lwd = 2)
py_require("tensorflow")
# Build LSTM model
model <- keras_model_sequential() %>%
layer_lstm(units = 50) %>%
layer_dense(units = 1)
# Compile model
model %>% compile(
loss = "mean_squared_error",
optimizer = "adam"
)
# Fit model
history <- model %>% fit(
X_train, y_train,
epochs = 50,
batch_size = 16,
validation_split = 0.2,
verbose = 0
)
# Make predictions
pred <- model %>% predict(X_test)
# Plot results
plot(y_test, type = "l", col = "blue", main = "LSTM Forecast vs Actual",
ylab = "Value", xlab = "Time")
lines(pred, col = "red")
legend("topleft", legend = c("Actual", "Predicted"), col = c("blue","red"), lty = 1)
library(tensorflow)
library(keras3)
# Generate sample time series (sine wave)
set.seed(123)
time <- 1:200
data <- sin(time * 0.1) + rnorm(200, sd = 0.1)
plot(time, data, type = "l", col = "blue",
main = "Simulated Time Series",
xlab = "Time", ylab = "Value")
# Prepare supervised data (windowed sequences)
lag <- 5   # number of time steps to look back
X <- t(sapply(1:(length(data)-lag), function(i) data[i:(i+lag-1)]))
y <- data[(lag+1):length(data)]
# Train-test split
train_size <- round(0.8 * nrow(X))
X_train <- X[1:train_size, 1:lag]
y_train <- y[1:train_size]
X_test  <- X[(train_size+1):nrow(X), 1:lag]
y_test  <- y[(train_size+1):length(y)]
# Reshape for LSTM [samples, timesteps, features]
X_train <- array(X_train, dim = c(nrow(X_train), lag, 1))
X_test  <- array(X_test,  dim = c(nrow(X_test), lag, 1))
# Plot the original data
plot(time, data, type = "l", col = "grey",
main = "Train-Test Split on Time Series",
xlab = "Time", ylab = "Value")
# Overlay training part
lines(time[(lag+1):(lag+length(y_train))], y_train,
col = "blue", lwd = 2)
# Overlay test part
lines(time[(lag+length(y_train)+1):(lag+length(y_train)+length(y_test))], y_test,
col = "red", lwd = 2)
# Add legend
legend("topleft", legend = c("Original", "Train", "Test"),
col = c("grey", "blue", "red"), lty = 1, lwd = 2)
py_require("tensorflow")
# Build LSTM model
model <- keras_model_sequential() %>%
layer_lstm(units = 50) %>%
layer_dense(units = 1)
# Compile model
model %>% compile(
loss = "mean_squared_error",
optimizer = "adam"
)
# Fit model
history <- model %>% fit(
X_train, y_train,
epochs = 50,
batch_size = 16,
validation_split = 0.2,
verbose = 0
)
# Make predictions
pred <- model %>% predict(X_test)
# Plot results
plot(y_test, type = "l", col = "blue", main = "LSTM Forecast vs Actual",
ylab = "Value", xlab = "Time")
lines(pred, col = "red")
legend("topleft", legend = c("Actual", "Predicted"), col = c("blue","red"), lty = 1)
# install.packages("tuner")
library(readr)
install.packages("tuner")
library(tuner)
install.packages("tuneR")
library(tuner)
install.packages("tuner")
R.version.string
install.packages("installr")
library(installr)
updateR()
install.packages("remotes")
remotes::install_github("EagerAI/kerastuneR")
library(kerastuneR)
install_kerastuner()
gc()
gc()
gc()
setwd("C:\\Users\\enton\\Project\\R\\CO2_forecasting")
library(readr)     # to read csv
library(dplyr)     # for data manipulation
library(lubridate) # to handle dates
library(forecast)  # ARIMA forecasting
library(ggplot2)   # plotting
library(tseries)
library(urca)
library(zoo)
library(prophet)
library(Metrics)
source("preprocessing.R")
source("stationary_test.R")
source("differencing_method.R")
source("ARIMA.R")
source("Prophet.R")
# Read dataset
df <- read_csv("co2_concentration.csv")
print(df)
str(df)
processed_data <- preprocess_data(df)
df_clean = processed_data$df
ts_data = processed_data$ts_data
# show the resulting monthly data frame to check the output
print(df_clean)
# Plot time series
ggplot(df_clean, aes(x = date, y = average)) +
geom_line(color = "steelblue") +
labs(title = "Monthly Avg CO2 Concentration from Jan 2010 to Dec 2023",
x = "date", y = "Avg Concentration")
# ggplot(df_clean, aes(x = average)) +
#   geom_histogram(fill = "skyblue", bins = 30, color = "black") +
#   labs(title = "Distribution of CO2 concentration",
#        x = "Avg Concentration", y = "Count")
stationary_test(df_clean)
differencing_method(df_clean)
Data <- df_clean$average
train_size <- floor(0.80 * length(Data))
train <- head(Data, train_size)
min_date <- min(df_clean$date)
min_year <- as.numeric(format(min_date, "%Y"))
min_month <- as.numeric(format(min_date, "%m"))
test  <- tail(Data, length(Data) - train_size)
ts_train <- ts(train,
frequency = 12,
start = c(min_year, min_month))
# Get the last date of the training set
last_train_date <- max(df_clean$date[1:length(train)])
first_test_date <- last_train_date %m+% months(1) # Find the date of the first observation in the test set
# Use the year and month of the first test date to set the start of the ts object
ts_test <- ts(test,
start = c(year(first_test_date), month(first_test_date)),
frequency = 12)
checkresiduals(ts_train)
ARIMA_method(ts_train, ts_test)
setwd("C:\\Users\\enton\\Project\\R\\CO2_forecasting")
library(readr)     # to read csv
library(dplyr)     # for data manipulation
library(lubridate) # to handle dates
library(forecast)  # ARIMA forecasting
library(ggplot2)   # plotting
library(tseries)
library(urca)
library(zoo)
library(prophet)
library(Metrics)
source("preprocessing.R")
source("stationary_test.R")
source("differencing_method.R")
source("ARIMA.R")
source("Prophet.R")
# Read dataset
df <- read_csv("co2_concentration.csv")
print(df)
str(df)
processed_data <- preprocess_data(df)
df_clean = processed_data$df
ts_data = processed_data$ts_data
# show the resulting monthly data frame to check the output
print(df_clean)
# Plot time series
ggplot(df_clean, aes(x = date, y = average)) +
geom_line(color = "steelblue") +
labs(title = "Monthly Avg CO2 Concentration from Jan 2010 to Dec 2023",
x = "date", y = "Avg Concentration")
# ggplot(df_clean, aes(x = average)) +
#   geom_histogram(fill = "skyblue", bins = 30, color = "black") +
#   labs(title = "Distribution of CO2 concentration",
#        x = "Avg Concentration", y = "Count")
stationary_test(df_clean)
differencing_method(df_clean)
Data <- df_clean$average
train_size <- floor(0.80 * length(Data))
train <- head(Data, train_size)
min_date <- min(df_clean$date)
min_year <- as.numeric(format(min_date, "%Y"))
min_month <- as.numeric(format(min_date, "%m"))
test  <- tail(Data, length(Data) - train_size)
ts_train <- ts(train,
frequency = 12,
start = c(min_year, min_month))
# Get the last date of the training set
last_train_date <- max(df_clean$date[1:length(train)])
first_test_date <- last_train_date %m+% months(1) # Find the date of the first observation in the test set
# Use the year and month of the first test date to set the start of the ts object
ts_test <- ts(test,
start = c(year(first_test_date), month(first_test_date)),
frequency = 12)
checkresiduals(ts_train)
ARIMA_method(ts_train, ts_test)
gc()
ARIMA_method <- function(ts_train, ts_test) {
ndiffs(ts_train)
nsdiffs(ts_train)
# -------------------------------
# Manual ARIMA
# -------------------------------
fit <- Arima(ts_train, order=c(2,1,0), seasonal=list(order =c(2,1,0), period=12))
checkresiduals(fit)
summary(fit)
# Forecast for the manual fit arima
fr <- forecast(fit, h = length(test))
layout(matrix(c(1,1)))
plot(fit)
layout(matrix(c(1,1)))
plot(fr)
lines(ts_test, col="turquoise2")
accuracy(fr, ts_test)
# -------------------------------
# Auto ARIMA
# -------------------------------
fit2 <- auto.arima(ts_train, seasonal = T, trace = T, approximation = F, ic="aic")
checkresiduals(fit2)
summary(fit2)
# Forecast for the auto fit arima
fr <- forecast(fit2, h = length(test))
layout(matrix(c(1,1)))
plot(fit2)
layout(matrix(c(1,1)))
plot(fr)
lines(ts_test, col="turquoise2")
accuracy(fr, ts_test)
}
ARIMA_method(ts_train, ts_test)
setwd("C:\\Users\\enton\\Project\\R\\CO2_forecasting")
library(readr)     # to read csv
library(dplyr)     # for data manipulation
library(lubridate) # to handle dates
library(forecast)  # ARIMA forecasting
library(ggplot2)   # plotting
library(tseries)
library(urca)
library(zoo)
library(prophet)
library(Metrics)
source("preprocessing.R")
source("stationary_test.R")
source("differencing_method.R")
source("ARIMA.R")
source("Prophet.R")
ARIMA_method <- function(ts_train, ts_test) {
ndiffs(ts_train)
nsdiffs(ts_train)
# -------------------------------
# Manual ARIMA
# -------------------------------
fit <- Arima(ts_train, order=c(2,1,0), seasonal=list(order =c(2,1,0), period=12))
checkresiduals(fit)
summary(fit)
# Forecast for the manual fit arima
fr <- forecast(fit, h = length(test))
layout(matrix(c(1,1)))
plot(fit)
layout(matrix(c(1,1)))
plot(fr)
lines(ts_test, col="turquoise2")
accuracy(fr, ts_test)
# -------------------------------
# Auto ARIMA
# -------------------------------
fit2 <- auto.arima(ts_train, seasonal = T, trace = T, approximation = F, ic="aic")
checkresiduals(fit2)
summary(fit2)
# Forecast for the auto fit arima
fr <- forecast(fit2, h = length(test))
layout(matrix(c(1,1)))
plot(fit2)
layout(matrix(c(1,1)))
plot(fr)
lines(ts_test, col="turquoise2")
accuracy(fr, ts_test)
}
setwd("C:\\Users\\enton\\Project\\R\\CO2_forecasting")
library(readr)     # to read csv
library(dplyr)     # for data manipulation
library(lubridate) # to handle dates
library(forecast)  # ARIMA forecasting
library(ggplot2)   # plotting
library(tseries)
library(urca)
library(zoo)
library(prophet)
library(Metrics)
source("preprocessing.R")
source("stationary_test.R")
source("differencing_method.R")
source("ARIMA.R")
source("Prophet.R")
# Read dataset
df <- read_csv("co2_concentration.csv")
print(df)
str(df)
processed_data <- preprocess_data(df)
df_clean = processed_data$df
ts_data = processed_data$ts_data
# show the resulting monthly data frame to check the output
print(df_clean)
# Plot time series
ggplot(df_clean, aes(x = date, y = average)) +
geom_line(color = "steelblue") +
labs(title = "Monthly Avg CO2 Concentration from Jan 2010 to Dec 2023",
x = "date", y = "Avg Concentration")
# ggplot(df_clean, aes(x = average)) +
#   geom_histogram(fill = "skyblue", bins = 30, color = "black") +
#   labs(title = "Distribution of CO2 concentration",
#        x = "Avg Concentration", y = "Count")
stationary_test(df_clean)
differencing_method(df_clean)
Data <- df_clean$average
train_size <- floor(0.80 * length(Data))
train <- head(Data, train_size)
min_date <- min(df_clean$date)
min_year <- as.numeric(format(min_date, "%Y"))
min_month <- as.numeric(format(min_date, "%m"))
test  <- tail(Data, length(Data) - train_size)
ts_train <- ts(train,
frequency = 12,
start = c(min_year, min_month))
# Get the last date of the training set
last_train_date <- max(df_clean$date[1:length(train)])
first_test_date <- last_train_date %m+% months(1) # Find the date of the first observation in the test set
# Use the year and month of the first test date to set the start of the ts object
ts_test <- ts(test,
start = c(year(first_test_date), month(first_test_date)),
frequency = 12)
checkresiduals(ts_train)
ARIMA_method(ts_train, ts_test)
setwd("C:\\Users\\enton\\Project\\R\\CO2_forecasting")
library(readr)     # to read csv
library(dplyr)     # for data manipulation
library(lubridate) # to handle dates
library(forecast)  # ARIMA forecasting
library(ggplot2)   # plotting
library(tseries)
library(urca)
library(zoo)
library(prophet)
library(Metrics)
source("preprocessing.R")
source("stationary_test.R")
source("differencing_method.R")
source("ARIMA.R")
source("Prophet.R")
# Read dataset
df <- read_csv("co2_concentration.csv")
print(df)
str(df)
processed_data <- preprocess_data(df)
df_clean = processed_data$df
ts_data = processed_data$ts_data
# show the resulting monthly data frame to check the output
print(df_clean)
# Plot time series
ggplot(df_clean, aes(x = date, y = average)) +
geom_line(color = "steelblue") +
labs(title = "Monthly Avg CO2 Concentration from Jan 2010 to Dec 2023",
x = "date", y = "Avg Concentration")
# ggplot(df_clean, aes(x = average)) +
#   geom_histogram(fill = "skyblue", bins = 30, color = "black") +
#   labs(title = "Distribution of CO2 concentration",
#        x = "Avg Concentration", y = "Count")
stationary_test(df_clean)
differencing_method(df_clean)
Data <- df_clean$average
train_size <- floor(0.80 * length(Data))
train <- head(Data, train_size)
min_date <- min(df_clean$date)
min_year <- as.numeric(format(min_date, "%Y"))
min_month <- as.numeric(format(min_date, "%m"))
test  <- tail(Data, length(Data) - train_size)
ts_train <- ts(train,
frequency = 12,
start = c(min_year, min_month))
# Get the last date of the training set
last_train_date <- max(df_clean$date[1:length(train)])
first_test_date <- last_train_date %m+% months(1) # Find the date of the first observation in the test set
# Use the year and month of the first test date to set the start of the ts object
ts_test <- ts(test,
start = c(year(first_test_date), month(first_test_date)),
frequency = 12)
checkresiduals(ts_train)
ARIMA_method(ts_train, ts_test)
setwd("C:\\Users\\enton\\Project\\R\\CO2_forecasting")
library(readr)     # to read csv
library(dplyr)     # for data manipulation
library(lubridate) # to handle dates
library(forecast)  # ARIMA forecasting
library(ggplot2)   # plotting
library(tseries)
library(urca)
library(zoo)
library(prophet)
library(Metrics)
source("preprocessing.R")
source("stationary_test.R")
source("differencing_method.R")
source("ARIMA.R")
source("Prophet.R")
# Read dataset
df <- read_csv("co2_concentration.csv")
print(df)
str(df)
processed_data <- preprocess_data(df)
df_clean = processed_data$df
ts_data = processed_data$ts_data
# show the resulting monthly data frame to check the output
print(df_clean)
# Plot time series
ggplot(df_clean, aes(x = date, y = average)) +
geom_line(color = "steelblue") +
labs(title = "Monthly Avg CO2 Concentration from Jan 2010 to Dec 2023",
x = "date", y = "Avg Concentration")
# ggplot(df_clean, aes(x = average)) +
#   geom_histogram(fill = "skyblue", bins = 30, color = "black") +
#   labs(title = "Distribution of CO2 concentration",
#        x = "Avg Concentration", y = "Count")
stationary_test(df_clean)
differencing_method(df_clean)
Data <- df_clean$average
train_size <- floor(0.80 * length(Data))
train <- head(Data, train_size)
min_date <- min(df_clean$date)
min_year <- as.numeric(format(min_date, "%Y"))
min_month <- as.numeric(format(min_date, "%m"))
test  <- tail(Data, length(Data) - train_size)
ts_train <- ts(train,
frequency = 12,
start = c(min_year, min_month))
# Get the last date of the training set
last_train_date <- max(df_clean$date[1:length(train)])
first_test_date <- last_train_date %m+% months(1) # Find the date of the first observation in the test set
# Use the year and month of the first test date to set the start of the ts object
ts_test <- ts(test,
start = c(year(first_test_date), month(first_test_date)),
frequency = 12)
checkresiduals(ts_train)
ARIMA_method(ts_train, ts_test)
ndiffs(ts_train)
nsdiffs(ts_train)
# -------------------------------
# Manual ARIMA
# -------------------------------
fit <- Arima(ts_train, order=c(2,1,0), seasonal=list(order =c(2,1,0), period=12))
checkresiduals(fit)
summary(fit)
# Forecast for the manual fit arima
fr <- forecast(fit, h = length(test))
layout(matrix(c(1,1)))
plot(fit)
layout(matrix(c(1,1)))
plot(fr)
lines(ts_test, col="turquoise2")
accuracy(fr, ts_test)
lines(ts_test, col="turquoise2")
accuracy(fr, ts_test)
fr
ts_test
length(fr)
legnth(ts_test)
accuracy(fr, ts_test)
# -------------------------------
# Auto ARIMA
# -------------------------------
fit2 <- auto.arima(ts_train, seasonal = T, trace = T, approximation = F, ic="aic")
accuracy(fr, ts_test)
lines(ts_test, col="turquoise2")
accuracy(fr, ts_test)
# -------------------------------
# Auto ARIMA
# -------------------------------
fit2 <- auto.arima(ts_train, seasonal = T, trace = T, approximation = F, ic="aic")
checkresiduals(fit2)
summary(fit2)
# Forecast for the auto fit arima
fr <- forecast(fit2, h = length(test))
layout(matrix(c(1,1)))
plot(fit2)
layout(matrix(c(1,1)))
plot(fr)
lines(ts_test, col="turquoise2")
accuracy(fr, ts_test)
accuracy(fr, ts_test)
packageVersion("forecast")
Prophet_method()
